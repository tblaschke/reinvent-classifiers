{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import re\n",
    "import os \n",
    "from butina import ParallelClusterData \n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download excape if it doesn't exists in current folder\n",
    "if not os.path.exists(\"pubchem.chembl.dataset4publication_inchi_smiles_v2.tsv.xz\"):\n",
    "    print(subprocess.check_output([\"wget\", \"-O\", \"pubchem.chembl.dataset4publication_inchi_smiles_v2.tsv.xz\", \"https://zenodo.org/record/2543724/files/pubchem.chembl.dataset4publication_inchi_smiles_v2.tsv.xz?download=1\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"DRD2\", \"HTR1A\"]\n",
    "activity_threshold = 7.0\n",
    "\n",
    "rnd_seed = 1234\n",
    "\n",
    "DIST_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True,fastmath=True)\n",
    "def _minmaxkernel_numba(data_1, data_2):\n",
    "    \"\"\"\n",
    "    MinMax kernel\n",
    "        K(x, y) = SUM_i min(x_i, y_i) / SUM_i max(x_i, y_i)\n",
    "    bounded by [0,1] as defined in:\n",
    "    \"Graph Kernels for Chemical Informatics\"\n",
    "    Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo and Pierre Baldi\n",
    "    Neural Networks\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.483&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result = np.zeros((data_1.shape[0], data_2.shape[0]), dtype=np.float64)\n",
    "\n",
    "    for i in prange(data_1.shape[0]):\n",
    "        for j in prange(data_2.shape[0]):\n",
    "            result[i,j] = _minmax_two_fp(data_1[i], data_2[j])\n",
    "    return result\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _minmax_two_fp(fp1, fp2):\n",
    "    common = numba.int32(0)\n",
    "    maxnum = numba.int32(0)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(fp1):\n",
    "        min_ = fp1[i]\n",
    "        max_ = fp2[i]\n",
    "\n",
    "        if min_ > max_:\n",
    "            min_ = fp2[i]\n",
    "            max_ = fp1[i]\n",
    "\n",
    "        common += min_\n",
    "        maxnum += max_\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return numba.float64(common) / numba.float64(maxnum)\n",
    "\n",
    "def counted_tanimoto_similarity(f1, fp2, return_distance=True):\n",
    "    if return_distance:\n",
    "        return 1. - _minmax_two_fp(fp1,fp2)\n",
    "    else:\n",
    "        return _minmax_two_fp(fp1,fp2)\n",
    "    \n",
    "def bulk_counted_tanimoto(fp1, fps, return_distance=True):\n",
    "    if return_distance:\n",
    "        return [1. - _minmax_two_fp(fp1,fp2) for fp2 in fps]\n",
    "    else: \n",
    "        return [_minmax_two_fp(fp1,fp2) for fp2 in fps]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(df, func):\n",
    "    df_split = np.array_split(df, cpu_count())\n",
    "    pool = Pool(cpu_count())\n",
    "    data = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_smiles(smi):\n",
    "    try:\n",
    "        return Chem.MolToSmiles(Chem.MolFromSmiles(smi),isomericSmiles=False)\n",
    "    except:\n",
    "        return np.NaN\n",
    "    \n",
    "def normalize_smiles_from_inchi(inchi):\n",
    "    try:\n",
    "        return Chem.MolToSmiles(Chem.MolFromInchi(inchi),isomericSmiles=False)\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "    \n",
    "def normalize_smiles_pandas(df):\n",
    "    return pd.Series([normalize_smiles(smi) for smi in df], index=df.index)\n",
    "\n",
    "def normalize_smiles_from_inchi_pandas(df):\n",
    "    return pd.Series([normalize_smiles_from_inchi(smi) for smi in df], index=df.index)\n",
    "\n",
    "def zgrep_data(f, string):\n",
    "    grep = 'grep'\n",
    "    if f.endswith(\".gz\"):\n",
    "        grep = 'zgrep'\n",
    "    if f.endswith(\".xz\"):\n",
    "        grep = 'xzgrep'\n",
    "    \n",
    "    if string == '':        \n",
    "        out = subprocess.check_output([grep, string, f])\n",
    "        grep_data = StringIO(out)\n",
    "        data = pd.read_csv(grep_data, sep='\\t')\n",
    "    else:\n",
    "        # read only the first row to get the columns\n",
    "        columns = pd.read_csv(f, sep='\\t', nrows=1, header=None).values.tolist()[0]    \n",
    "\n",
    "        out = subprocess.check_output([grep, string, f]).decode(\"UTF-8\")\n",
    "        grep_data = StringIO(out)\n",
    "\n",
    "        data = pd.read_csv(grep_data, sep='\\t', names=columns, header=None)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_data(target):\n",
    "    print(\"    read excape\")\n",
    "    excape = zgrep_data(\"pubchem.chembl.dataset4publication_inchi_smiles_v2.tsv.xz\", target).set_index('Ambit_InchiKey')\n",
    "    excape = excape[excape.Gene_Symbol == target]\n",
    "    excape = excape.fillna(value={\"pXC50\": 0.0})\n",
    "    excape = excape[[\"Original_Entry_ID\",\"Entrez_ID\", \"pXC50\", \"DB\", \"Gene_Symbol\", \"InChI\"]]\n",
    "    print(\"    normalize SMILES\")\n",
    "    RDKIT_SMILES = applyParallel(excape[\"InChI\"], normalize_smiles_from_inchi_pandas).rename(\"RDKIT_SMILES\")\n",
    "    subset = pd.concat([excape, RDKIT_SMILES], axis=1, join='inner')\n",
    "    subset = subset.dropna(subset=[\"RDKIT_SMILES\"])\n",
    "    #we sort by activity and keep the lowest activity value for duplicates\n",
    "    subset = subset.sort_values(by=\"pXC50\", ascending=True).drop_duplicates(subset=\"RDKIT_SMILES\", keep='first')\n",
    "    return subset\n",
    "\n",
    "def cecfp6_from_mol(mol):\n",
    "    fp = AllChem.GetMorganFingerprint(mol, 3, useCounts=True, useFeatures=False)\n",
    "    size = 2048\n",
    "    nfp = np.zeros(size, np.int32)\n",
    "    for idx,v in fp.GetNonzeroElements().items():\n",
    "        nidx = idx%size\n",
    "        nfp[nidx] += int(v)\n",
    "    return nfp\n",
    "\n",
    "def generate_fingerprints(df):\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in df]\n",
    "    cfp = [cecfp6_from_mol(mol) for mol in mols]\n",
    "    return pd.DataFrame({\"cfp\":cfp}, index=df.index)\n",
    "    \n",
    "def create_training_test_validation(df, activity_threshold, training_actives=0.6, test_actives=0.20, validation_actives=0.20, force_specific_smiles_into_test=[]):\n",
    "    print(\"  assign inactives\")\n",
    "    inactives = df.query('pXC50 < @activity_threshold')\n",
    "    print(f\"\\t\\tNumber of inactives: {len(inactives)}\")\n",
    "    if len(inactives) < 100000:\n",
    "        print(\"\\t\\t\\tless than 100000 inactives. Using only: {}\".format(len(inactives)))\n",
    "        inactives = inactives.sample(n=len(inactives), replace=False, random_state=rnd_seed)\n",
    "    else:\n",
    "        inactives = inactives.sample(n=100000, replace=False, random_state=rnd_seed)\n",
    "    inactives_training = inactives.sample(frac=training_actives, replace=False, random_state=rnd_seed).index\n",
    "    inactives_test = inactives.drop(inactives_training).sample(n=int(round(len(inactives)*test_actives)), replace=False, random_state=rnd_seed).index\n",
    "    inactives_validation = inactives.drop(inactives_training).drop(inactives_test).index\n",
    "    \n",
    "    df.loc[(inactives_training,\"trainingset_class\")] = \"training\"\n",
    "    df.loc[(inactives_test,\"trainingset_class\")] = \"test\"\n",
    "    df.loc[(inactives_validation,\"trainingset_class\")] = \"validation\"\n",
    "    \n",
    "    actives = df.query('pXC50 >= @activity_threshold')\n",
    "    print(f\"\\t\\tNumber of actives: {len(actives)}\")\n",
    "    actives = actives.sample(n=len(actives), replace=False, random_state=rnd_seed)\n",
    "    \n",
    "    def cluster_contains_compounds_to_filter(cluster, force_specific_smiles_into_test):\n",
    "        if len(force_specific_smiles_into_test) == 0:\n",
    "            return False\n",
    "        for id_ in cluster:\n",
    "            if actives.iloc[id_][\"RDKIT_SMILES\"] in force_specific_smiles_into_test:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    print(\"  cluster actives\")\n",
    "    clusters = ParallelClusterData(actives[\"cfp\"].values, len(actives), DIST_THRESHOLD, distFunc=bulk_counted_tanimoto, reordering=True)\n",
    "    print(\"  assign actives\")\n",
    "    counter = 0\n",
    "    \n",
    "    nb_test = 0\n",
    "    nb_validation = 0\n",
    "    nb_training = 0\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        if cluster_contains_compounds_to_filter(cluster, force_specific_smiles_into_test):\n",
    "            trainingset_class = \"test\"\n",
    "            nb_test += len(cluster)\n",
    "        else:        \n",
    "            \n",
    "            progress_test = nb_test/(len(actives)*test_actives)\n",
    "            progress_validation = nb_validation/(len(actives)*validation_actives)\n",
    "            progress_training = nb_training/(len(actives)*training_actives)\n",
    "            \n",
    "            progresses = [progress_test,progress_validation,progress_training]\n",
    "            idx = progresses.index(min(progresses))\n",
    "            if idx == 0:\n",
    "                trainingset_class = \"test\"\n",
    "                nb_test += len(cluster)\n",
    "            elif idx == 1:\n",
    "                trainingset_class = \"validation\"\n",
    "                nb_validation += len(cluster)\n",
    "            elif idx == 2:\n",
    "                trainingset_class = \"training\"  \n",
    "                nb_training += len(cluster)\n",
    "                \n",
    "        for id_ in cluster:\n",
    "            index =  actives.iloc[id_].name\n",
    "            df.loc[(index,'trainingset_class')] = trainingset_class\n",
    "            df.loc[(index,'cluster_id')] = cluster_id\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process DRD2\n",
      "  grep data\n",
      "    read excape\n",
      "    normalize SMILES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [16:16:21] Explicit valence for atom # 19 N, 5, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  generate fingerprints\n",
      "  assign inactives\n",
      "\t\tNumber of inactives: 346206\n",
      "\t\tNumber of actives: 2981\n",
      "  cluster actives\n",
      "  assign actives\n",
      "  save dataframe to DRD2_df.pkl.gz\n",
      "Process HTR1A\n",
      "  grep data\n",
      "    read excape\n",
      "    normalize SMILES\n",
      "  generate fingerprints\n",
      "  assign inactives\n",
      "\t\tNumber of inactives: 66684\n",
      "\t\t\tless than 100000 inactives. Using only: 66684\n",
      "\t\tNumber of actives: 3599\n",
      "  cluster actives\n",
      "  assign actives\n",
      "  save dataframe to HTR1A_df.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    def do(target):\n",
    "        print(\"Process {}\".format(target))\n",
    "        print(\"  grep data\")\n",
    "        excape_subset = get_data(target)\n",
    "        print(\"  generate fingerprints\")\n",
    "        fingerprints  = applyParallel(excape_subset[\"RDKIT_SMILES\"], generate_fingerprints)\n",
    "        excape_subset = pd.concat([excape_subset, fingerprints], axis=1, join='inner')\n",
    "        excape_subset[\"trainingset_class\"] = None\n",
    "        excape_subset[\"cluster_id\"] = None\n",
    "\n",
    "        exclude_smiles = []\n",
    "        target_df = excape_subset[excape_subset.Gene_Symbol == target].sample(frac=1. ,random_state=rnd_seed).copy()\n",
    "        target_df = create_training_test_validation(target_df, activity_threshold=activity_threshold, force_specific_smiles_into_test=exclude_smiles)\n",
    "        target_df = target_df.dropna(subset=[\"trainingset_class\"])\n",
    "        target_df[\"trainingset_class\"] = target_df[\"trainingset_class\"].astype('category')\n",
    "        target_df[\"cluster_id\"] =  target_df[\"cluster_id\"].fillna(-1).astype('int64')\n",
    "        target_df[\"activity_label\"] = (target_df[\"pXC50\"] >= activity_threshold).astype('int')\n",
    "        target_df = target_df.sample(frac=1. ,random_state=rnd_seed)\n",
    "        print(\"  save dataframe to {}_df.pkl.gz\".format(target))\n",
    "        target_df.to_pickle(target+\"_df.pkl.gz\")\n",
    "    do(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
