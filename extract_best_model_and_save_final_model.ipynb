{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "import time\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, recall_score, precision_score, matthews_corrcoef, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn import calibration, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "def tanimotokernel(data_1, data_2):\n",
    "    if isinstance(data_1, scipy.sparse.csr_matrix) and isinstance(data_2, scipy.sparse.csr_matrix):\n",
    "        return _sparse_tanimotokernel(data_1, data_2)\n",
    "    elif isinstance(data_1, scipy.sparse.csr_matrix) or isinstance(data_2, scipy.sparse.csr_matrix):\n",
    "        # try to sparsify the input\n",
    "        return _sparse_tanimotokernel(scipy.sparse.csr_matrix(data_1), scipy.sparse.csr_matrix(data_2))\n",
    "    else:  # both are dense\n",
    "        return _dense_tanimotokernel(data_1, data_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def _dense_tanimotokernel(data_1, data_2):\n",
    "    \"\"\"\n",
    "    Tanimoto kernel\n",
    "        K(x, y) = <x, y> / (||x||^2 + ||y||^2 - <x, y>)\n",
    "    as defined in:\n",
    "    \"Graph Kernels for Chemical Informatics\"\n",
    "    Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo and Pierre Baldi\n",
    "    Neural Networks\n",
    "    https://www.sciencedirect.com/science/article/pii/S0893608005001693\n",
    "    http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Ralaivola2005Graph.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    norm_1 = (data_1 ** 2).sum(axis=1).reshape(data_1.shape[0], 1)\n",
    "    norm_2 = (data_2 ** 2).sum(axis=1).reshape(data_2.shape[0], 1)\n",
    "    prod = data_1.dot(data_2.T)\n",
    "\n",
    "    divisor = (norm_1 + norm_2.T - prod) + np.finfo(data_1.dtype).eps\n",
    "    return prod / divisor\n",
    "\n",
    "\n",
    "\n",
    "def _sparse_tanimotokernel(data_1, data_2):\n",
    "    \"\"\"\n",
    "    Tanimoto kernel\n",
    "        K(x, y) = <x, y> / (||x||^2 + ||y||^2 - <x, y>)\n",
    "    as defined in:\n",
    "    \"Graph Kernels for Chemical Informatics\"\n",
    "    Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo and Pierre Baldi\n",
    "    Neural Networks\n",
    "    https://www.sciencedirect.com/science/article/pii/S0893608005001693\n",
    "    http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Ralaivola2005Graph.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    norm_1 = np.array(data_1.power(2).sum(axis=1).reshape(data_1.shape[0], 1))\n",
    "    norm_2 = np.array(data_2.power(2).sum(axis=1).reshape(data_2.shape[0], 1))\n",
    "    prod = data_1.dot(data_2.T).A\n",
    "\n",
    "    divisor = (norm_1 + norm_2.T - prod) + np.finfo(data_1.dtype).eps\n",
    "    result = prod / divisor\n",
    "    return result\n",
    "\n",
    "def _minmaxkernel_numpy(data_1, data_2):\n",
    "    \"\"\"\n",
    "    MinMax kernel\n",
    "        K(x, y) = SUM_i min(x_i, y_i) / SUM_i max(x_i, y_i)\n",
    "    bounded by [0,1] as defined in:\n",
    "    \"Graph Kernels for Chemical Informatics\"\n",
    "    Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo and Pierre Baldi\n",
    "    Neural Networks\n",
    "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.483&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "    return np.stack([(np.minimum(data_1, data_2[cpd,:]).sum(axis=1) / np.maximum(data_1, data_2[cpd,:]).sum(axis=1))  for cpd in range(data_2.shape[0])],axis=1)\n",
    "\n",
    "\n",
    "try: \n",
    "    import numba\n",
    "    from numba import njit, prange\n",
    "    \n",
    "    @njit(parallel=True,fastmath=True)\n",
    "    def _minmaxkernel_numba(data_1, data_2):\n",
    "        \"\"\"\n",
    "        MinMax kernel\n",
    "            K(x, y) = SUM_i min(x_i, y_i) / SUM_i max(x_i, y_i)\n",
    "        bounded by [0,1] as defined in:\n",
    "        \"Graph Kernels for Chemical Informatics\"\n",
    "        Liva Ralaivola, Sanjay J. Swamidass, Hiroto Saigo and Pierre Baldi\n",
    "        Neural Networks\n",
    "        http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.483&rep=rep1&type=pdf\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        result = np.zeros((data_1.shape[0], data_2.shape[0]), dtype=np.float64)\n",
    "\n",
    "        for i in prange(data_1.shape[0]):\n",
    "            for j in prange(data_2.shape[0]):\n",
    "                result[i,j] = _minmax_two_fp(data_1[i], data_2[j])\n",
    "        return result\n",
    "\n",
    "\n",
    "    @njit(fastmath=True)\n",
    "    def _minmax_two_fp(fp1, fp2):\n",
    "        common = numba.int32(0)\n",
    "        maxnum = numba.int32(0)\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(fp1):\n",
    "            min_ = fp1[i]\n",
    "            max_ = fp2[i]\n",
    "\n",
    "            if min_ > max_:\n",
    "                min_ = fp2[i]\n",
    "                max_ = fp1[i]\n",
    "\n",
    "            common += min_\n",
    "            maxnum += max_\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "        return numba.float64(common) / numba.float64(maxnum)\n",
    "\n",
    "    minmaxkernel = _minmaxkernel_numba\n",
    "    \n",
    "except:\n",
    "    \n",
    "    print(\"Couldn't find numba. I suggest to install numba to compute the minmax kernel much much faster\")\n",
    "    \n",
    "    minmaxkernel = _minmaxkernel_numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target, random_state=None):\n",
    "    df = pd.read_pickle(\"{}_df.pkl.gz\".format(target))\n",
    "    if random_state:\n",
    "        df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    training = df[df.trainingset_class == \"training\"]\n",
    "    test = df[df.trainingset_class == \"test\"]\n",
    "    validation = df[df.trainingset_class == \"validation\"]\n",
    "\n",
    "    training_X = np.array([np.array(e) for e in training.cfp.values],dtype=np.float64, order='C')\n",
    "    training_Y = np.array(training.activity_label, dtype=np.float64, order='C')\n",
    "\n",
    "    test_X = np.array([np.array(e) for e in test.cfp.values],dtype=np.float64, order='C')\n",
    "    test_Y = np.array(test.activity_label, dtype=np.float64, order='C')\n",
    "\n",
    "    validation_X = np.array([np.array(e) for e in validation.cfp.values],dtype=np.float64, order='C')\n",
    "    validation_Y = np.array(validation.activity_label, dtype=np.float64, order='C')\n",
    "\n",
    "    return training_X, training_Y, test_X, test_Y, validation_X, validation_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stats(file):    \n",
    "    scores = {}\n",
    "    with open(file) as fd:\n",
    "        lines = fd.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if len(splits) == 3:\n",
    "                set_, scorename, score = splits\n",
    "            else:\n",
    "                set_, scorename, threshold, score = splits\n",
    "                scorename = f'{scorename} {threshold}'\n",
    "                \n",
    "            scorename = scorename[:-1] #remove the : sign\n",
    "            score = float(score)\n",
    "            if set_ not in scores:\n",
    "                scores[set_] = {}\n",
    "            scores[set_][scorename] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                                              \tACC\tAUC\tF1\tMCC\n",
      "models/DRD2_c_0.1_kernel_minmax_balanced_proba.stats\t0.99\t0.998\t0.77\t0.79\n",
      "models/DRD2_c_0.1_kernel_minmax_balanced_proba.stats\t0.93\t0.989\t0.7\t0.71\n",
      "models/DRD2_c_0.1_kernel_minmax_balanced_proba.stats\t0.96\t0.9837\t0.71\t0.72\n",
      "\n",
      "Refit\n",
      "Name                                              \tACC\tAUC\tF1\tMCC\n",
      "models/HTR1A_c_0.1_kernel_minmax_balanced_proba.stats\t0.98\t0.9948\t0.77\t0.78\n",
      "models/HTR1A_c_0.1_kernel_minmax_balanced_proba.stats\t0.96\t0.9877\t0.74\t0.75\n",
      "models/HTR1A_c_0.1_kernel_minmax_balanced_proba.stats\t0.96\t0.9897\t0.75\t0.75\n",
      "\n",
      "Refit\n"
     ]
    }
   ],
   "source": [
    "for target in ['DRD2','HTR1A']:\n",
    "    files = {}\n",
    "    for file in glob.glob(\"models/{}*.stats\".format(target)):\n",
    "        files[file] = parse_stats(file)\n",
    "\n",
    "    best_clf = None\n",
    "    best_score = 0\n",
    "\n",
    "    scorename = \"F1\"\n",
    "    setname = 'Validation set'\n",
    "    for key,value in files.items():\n",
    "        if value[setname][scorename] > best_score:\n",
    "            best_score = value[setname][scorename]\n",
    "            best_clf = key\n",
    "\n",
    "    \n",
    "    target_name,c,cvalue,kernel,kernelname,class_weight,suffix = best_clf.split(\"_\")\n",
    "    \n",
    "    cvalue = float(cvalue)\n",
    "    \n",
    "    if kernelname == 'tanimoto':\n",
    "        kernel = tanimotokernel\n",
    "    elif kernelname == 'minmax':\n",
    "        kernel = minmaxkernel\n",
    "    else:\n",
    "        kernel = kernelname\n",
    "        \n",
    "    if class_weight == 'unbalanced':\n",
    "        class_weight = None\n",
    "\n",
    "    print(f'{\"Name                                              \":30}\\t{\"ACC\"}\\t{\"AUC\"}\\t{\"F1\"}\\t{\"MCC\"}')\n",
    "    setname = 'Training set'\n",
    "    print(f'{best_clf:30}\\t{files[best_clf][setname][\"Balanced Accuracy\"]:.2}\\t{files[best_clf][setname][\"ROC AUC\"]:.4}\\t{files[best_clf][setname][\"F1\"]:.2}\\t{files[best_clf][setname][\"MCC\"]:.2}')\n",
    "    setname = 'Validation set'\n",
    "    print(f'{best_clf:30}\\t{files[best_clf][setname][\"Balanced Accuracy\"]:.2}\\t{files[best_clf][setname][\"ROC AUC\"]:.4}\\t{files[best_clf][setname][\"F1\"]:.2}\\t{files[best_clf][setname][\"MCC\"]:.2}')\n",
    "    setname = 'Test set'\n",
    "    print(f'{best_clf:30}\\t{files[best_clf][setname][\"Balanced Accuracy\"]:.2}\\t{files[best_clf][setname][\"ROC AUC\"]:.4}\\t{files[best_clf][setname][\"F1\"]:.2}\\t{files[best_clf][setname][\"MCC\"]:.2}')\n",
    "    print()\n",
    "    \n",
    "    training_X, training_Y, test_X, test_Y, validation_X, validation_Y = load_data(target) \n",
    "\n",
    "    \n",
    "    clf = svm.SVC(C=cvalue, random_state=1234, kernel=kernel, cache_size=1900 ,probability=True, class_weight=class_weight)\n",
    "    print(\"Refit\")\n",
    "    clf.fit(training_X, training_Y)\n",
    "    \n",
    "    def score_clf(clf):\n",
    "        sets = {\"Training set\": (training_X, training_Y) ,\"Test set\": (test_X, test_Y), \"validation set\": (validation_X, validation_Y) }\n",
    "        scores_binary = {\"Balanced Accuracy\": lambda x,y: balanced_accuracy_score(x,y, adjusted=False), \"Recall\": recall_score, \"Precision\":  precision_score, \"MCC\": matthews_corrcoef, \"F1\": f1_score }\n",
    "        scores_proba = {\"ROC AUC\": roc_auc_score }\n",
    "\n",
    "        scores = {}\n",
    "        for setname, data in sets.items():\n",
    "            scores[setname] = {}\n",
    "            data_X = data[0]\n",
    "            data_Y = data[1]\n",
    "\n",
    "            predicted_Y = clf.predict(data_X)\n",
    "            predicted_Y_proba = clf.predict_proba(data_X)[:,1]\n",
    "\n",
    "            for scorename, score_fn in scores_binary.items():\n",
    "                scores[setname][scorename] = score_fn(data_Y, predicted_Y)\n",
    "\n",
    "            for scorename, score_fn in scores_proba.items():\n",
    "                scores[setname][scorename] = score_fn(data_Y, predicted_Y_proba)\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    clf.kernel = kernelname\n",
    "    joblib.dump(clf, \"{}_final.pkl\".format(target), compress=(\"xz\",9), protocol=-1)\n",
    "    clf.kernel = kernel\n",
    "    \n",
    "#     print(\"Scoring: ...\")\n",
    "#     scores = score_clf(clf)\n",
    "    \n",
    "#     if scores.items() == files[best_clf].items():\n",
    "#         print(\" All metrics are the same.\")\n",
    "#     else:\n",
    "#         print(\"Old Values: \")\n",
    "#         print(scores.items())\n",
    "#         print(\"New Values: \")\n",
    "#         print(files[best_clf].items())\n",
    "    \n",
    "#     #calibrate the classifier using platt scaling\n",
    "    \n",
    "#     # we calibrate using the training and test set\n",
    "#     train_test_X = np.concatenate([training_X,test_X], axis=0)\n",
    "#     train_test_Y = np.concatenate([training_Y,test_Y], axis=0)\n",
    "#     sample_weights_train_test = compute_sample_weight(class_weight, train_test_Y)\n",
    "    \n",
    "#     print(\"Calibrate\")\n",
    "#     clf_calibrated = calibration.CalibratedClassifierCV(base_estimator=clf, method='sigmoid', cv= 'prefit')\n",
    "#     clf_calibrated.fit(train_test_X, train_test_Y, sample_weight=sample_weights_train_test)\n",
    "        \n",
    "#     for c in clf_calibrated.calibrated_classifiers_:\n",
    "#         c.base_estimator.kernel = kernelname\n",
    "#     joblib.dump(clf_calibrated, \"/Users/thomas/projects/reinvent-classifiers/{}_final.pkl\".format(target), compress=(\"xz\",9), protocol=-1)\n",
    "#     for c in clf_calibrated.calibrated_classifiers_:\n",
    "#         c.base_estimator.kernel = kernel\n",
    "    \n",
    "#     def score_clf_print(clf, thresholds=[0.5]):\n",
    "#         sets = {\"Training set\": (training_X, training_Y) ,\"Test set\": (test_X, test_Y), \"validation set\": (validation_X, validation_Y) }\n",
    "#         scores_binary = {\"Balanced Accuracy\": lambda x,y: balanced_accuracy_score(x,y, adjusted=False), \"Recall\": recall_score, \"Precision\":  precision_score, \"MCC\": matthews_corrcoef, \"F1\": f1_score }\n",
    "#         scores_proba = {\"ROC AUC\": roc_auc_score }\n",
    "\n",
    "#         scores = {}\n",
    "#         for setname, data in sets.items():\n",
    "#             data_X = data[0]\n",
    "#             data_Y = data[1]\n",
    "\n",
    "#             predicted_Y_proba = clf.predict_proba(data_X)[:,1]\n",
    "#             for threshold in thresholds:\n",
    "#                 if threshold not in scores:\n",
    "#                     scores[threshold] = {}\n",
    "\n",
    "\n",
    "#                 predicted_Y = np.array(predicted_Y_proba > threshold, dtype=np.float)\n",
    "\n",
    "#                 for scorename, score_fn in scores_binary.items():\n",
    "#                     scores[threshold][\"{}\\t{}\".format(setname, scorename)] = score_fn(data_Y, predicted_Y)\n",
    "\n",
    "#                 for scorename, score_fn in scores_proba.items():\n",
    "#                     scores[threshold][\"{}\\t{}\".format(setname, scorename)] = score_fn(data_Y, predicted_Y_proba)\n",
    "\n",
    "#         return scores\n",
    "\n",
    "#     print(\"Scoring for print: ...\")\n",
    "#     scores_thres = score_clf_print(clf_calibrated, thresholds=[0.5,0.6,0.7,0.8,0.85,0.9])\n",
    "\n",
    "\n",
    "#     for threshold, scores in scores_thres.items(): \n",
    "#         with open('/Users/thomas/projects/reinvent-classifiers/{}_final_calibratedcv.stats.{}'.format(target,threshold), \"w\") as fd:\n",
    "#                 for scorename, score in scores.items():\n",
    "#                     line = \"{}:\\t{}\\n\".format(scorename, score)\n",
    "#                     fd.write(line)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in ['DRD2','HTR1A']:   \n",
    "    training_X, training_Y, test_X, test_Y, validation_X, validation_Y = load_data(target) \n",
    "\n",
    "    \n",
    "    clf = joblib.load(f'{target}_final.pkl')\n",
    "    \n",
    "    if clf.kernel == 'tanimoto':\n",
    "        clf.kernel = tanimotokernel\n",
    "    elif clf.kernel == 'minmax':\n",
    "        clf.kernel = minmaxkernel\n",
    "    else:\n",
    "        clf.kernel = clf.kernel\n",
    "        \n",
    "    \n",
    "    def score_clf(clf):\n",
    "        sets = {\"Training set\": (training_X, training_Y) ,\"Test set\": (test_X, test_Y), \"Validation set\": (validation_X, validation_Y) }\n",
    "        scores_binary = {\"Balanced Accuracy\": lambda x,y: balanced_accuracy_score(x,y, adjusted=False), \"Recall\": recall_score, \"Precision\":  precision_score, \"MCC\": matthews_corrcoef, \"F1\": f1_score }\n",
    "        scores_proba = {\"ROC AUC\": roc_auc_score }\n",
    "\n",
    "        scores = {}\n",
    "        for setname, data in sets.items():\n",
    "            data_X = data[0]\n",
    "            data_Y = data[1]\n",
    "            \n",
    "            predicted_Y = clf.predict(data_X)\n",
    "            predicted_Y_proba = clf.predict_proba(data_X)[:,1]\n",
    "            for scorename, score_fn in scores_binary.items():\n",
    "                scores[\"{}\\t{}\".format(setname, scorename)] = score_fn(data_Y, predicted_Y)\n",
    "\n",
    "            for scorename, score_fn in scores_proba.items():\n",
    "                scores[\"{}\\t{}\".format(setname, scorename)] = score_fn(data_Y, predicted_Y_proba)\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    scores = score_clf(clf)\n",
    "    \n",
    "    with open(f'{target}_final.stats', \"w\") as fd:\n",
    "            for scorename, score in scores.items():\n",
    "                line = \"{}:\\t{}\\n\".format(scorename, score)\n",
    "                fd.write(line)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
